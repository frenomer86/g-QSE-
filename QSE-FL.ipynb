{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf11a999-b0e5-4dca-a359-a6367008f765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 17:50:27.597469: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-29 17:50:28.077801: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-29 17:50:28.077858: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-29 17:50:28.079561: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-29 17:50:28.392770: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-29 17:50:28.394975: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-29 17:50:30.383125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, mean_absolute_error \n",
    "from tensorflow.keras import layers, models, applications\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D, Multiply, concatenate, Input, BatchNormalization, ReLU, Add, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import keras.backend as K\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Imports\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from glob import glob\n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.models import load_model\n",
    "from keras.losses import binary_crossentropy \n",
    "from tensorflow.keras.layers import ReLU, Add  \n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import Sequential \n",
    "#import albumentations as A\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "import matplotlib.cm as cm \n",
    "import tenseal as ts\n",
    "import random\n",
    "import time\n",
    " \n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(seed=42)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb7e2bc-e055-4bd5-98af-32999b708788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mock training, validation, and test data\n",
    "\n",
    "# Image size and other constants\n",
    "IMG_SIZE = 240\n",
    "batch_size = 32\n",
    "INPUT_SHAPE = (IMG_SIZE, IMG_SIZE, 3) \n",
    "num_clients=5\n",
    "epochs=100\n",
    "#num_rounds=1\n",
    "#local_epochs=1\n",
    "\n",
    "train_image_dir = \"/home/freno/Desktop/THESIS/JAJA-TA/images\"\n",
    "train_mask_dir = \"/home/freno/Desktop/THESIS/JAJA-TA/masks\"\n",
    "\n",
    "\n",
    "test_image_dir = \"/home/freno/Desktop/THESIS/JAJA-TA/TEST_ALL/general_test/images\"\n",
    "test_mask_dir = \"/home/freno/Desktop/THESIS/JAJA-TA/TEST_ALL/general_test/masks\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c17f302e-0706-499b-be46-9c8dc4a7c814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, image_paths, mask_paths, target_shape=(IMG_SIZE, IMG_SIZE), augment=False):\n",
    "        self.image_paths = sorted(image_paths)\n",
    "        self.mask_paths = sorted(mask_paths)\n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices((self.image_paths, self.mask_paths))\n",
    "        self.AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "        self.target_shape = target_shape\n",
    "        self.augment = augment  # Set augment flag\n",
    "\n",
    "    @tf.function\n",
    "    def parse_images(self, image_path, mask_path):\n",
    "        # Read and preprocess the image\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.io.decode_jpeg(image, channels=3)\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = tf.image.resize(image, self.target_shape) / 255.\n",
    "\n",
    "        # Read and preprocess the mask\n",
    "        mask = tf.io.read_file(mask_path)\n",
    "        mask = tf.io.decode_jpeg(mask, channels=1)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        mask = tf.image.resize(mask, self.target_shape) / 255.\n",
    "\n",
    "        # Apply data augmentation if augment flag is True\n",
    "        if self.augment:\n",
    "            image, mask = self.augment_image_mask(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    @tf.function \n",
    "    def data_generator(self, batch_size=batch_size, shuffle=True, repeat=False):\n",
    "        dataset = self.dataset\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=len(self.image_paths))\n",
    "    \n",
    "        dataset = dataset.map(self.parse_images, num_parallel_calls=self.AUTOTUNE)\n",
    "    \n",
    "        if repeat:\n",
    "            dataset = dataset.repeat()  # Use repeat only for training\n",
    "    \n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(buffer_size=self.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def augment_image_mask(self, image, mask):\n",
    "        \"\"\"Apply data augmentation to image and mask.\"\"\"\n",
    "        # Random flip (horizontal and/or vertical)\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            image = tf.image.flip_left_right(image)\n",
    "            mask = tf.image.flip_left_right(mask)\n",
    "\n",
    "        if tf.random.uniform(()) > 0.5:\n",
    "            image = tf.image.flip_up_down(image)\n",
    "            mask = tf.image.flip_up_down(mask)\n",
    "\n",
    "        # Random rotation (90 degrees increments)\n",
    "        k = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n",
    "        image = tf.image.rot90(image, k)\n",
    "        mask = tf.image.rot90(mask, k)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "def prepare_federated_data(image_dir, mask_dir, num_clients=5, target_shape=(IMG_SIZE, IMG_SIZE), augment=True):\n",
    "    # Get image and mask paths\n",
    "    image_paths = sorted([os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    mask_paths = sorted([os.path.join(mask_dir, fname) for fname in os.listdir(mask_dir) if fname.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    \n",
    "    # Initialize the DataLoader with all paths\n",
    "    total_loader = DataLoader(image_paths, mask_paths, target_shape=target_shape, augment=augment)\n",
    "    \n",
    "    # Get the total dataset\n",
    "    total_dataset = total_loader.dataset\n",
    "    total_size = len(image_paths)\n",
    "    \n",
    "    # Shuffle the dataset before splitting\n",
    "    total_dataset = total_dataset.shuffle(buffer_size=total_size)\n",
    "    \n",
    "    # Split the dataset into num_clients subsets\n",
    "    client_datasets = []\n",
    "    split_size = total_size // num_clients\n",
    "    for i in range(num_clients):\n",
    "        start_idx = i * split_size\n",
    "        end_idx = (i + 1) * split_size if i != num_clients - 1 else total_size\n",
    "        \n",
    "        client_image_paths = image_paths[start_idx:end_idx]\n",
    "        client_mask_paths = mask_paths[start_idx:end_idx]\n",
    "        \n",
    "        client_loader = DataLoader(client_image_paths, client_mask_paths, target_shape=target_shape, augment=augment)\n",
    "        client_datasets.append(client_loader)\n",
    "\n",
    "    return client_datasets\n",
    "\n",
    "# Prepare the federated datasets for training\n",
    "train_client_datasets = prepare_federated_data(train_image_dir, train_mask_dir, num_clients=num_clients, augment=True)\n",
    "test_client_datasets = prepare_federated_data(test_image_dir, test_mask_dir, num_clients=num_clients, augment=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8a477c5-879a-49bd-8157-03fa3f3b8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def conv_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    \"\"\"Basic convolutional block with two Conv2D layers and batch normalization.\"\"\"\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
    "    c = BatchNormalization()(c)\n",
    "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
    "    c = BatchNormalization()(c)\n",
    "    return c\n",
    "\n",
    "def down_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    \"\"\"Downsampling block: Conv2D + Conv2D + MaxPooling2D.\"\"\"\n",
    "    c = conv_block(x, filters, kernel_size, padding, strides)\n",
    "    p = MaxPooling2D((2, 2), (2, 2))(c)\n",
    "    return c, p\n",
    "\n",
    "def up_block(x, skip, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    \"\"\"Upsampling block: Upsample + Conv2D + Concatenate with skip connections.\"\"\"\n",
    "    us = UpSampling2D((2, 2))(x)\n",
    "    concat = Concatenate()([us, skip])\n",
    "    c = conv_block(concat, filters, kernel_size, padding, strides)\n",
    "    return c\n",
    "\n",
    "def UNet():\n",
    "    f = [4, 8, 16, 32, 64]  # Filter sizes for each level\n",
    "    inputs = Input((240, 240, 3))\n",
    "\n",
    "    # Downsampling path (Encoder)\n",
    "    c1, t1 = down_block(inputs, f[0])\n",
    "    c2, t2 = down_block(t1, f[1])\n",
    "    c3, t3 = down_block(t2, f[2])\n",
    "    c4, t4 = down_block(t3, f[3])\n",
    "\n",
    "    # Bottleneck\n",
    "    bn = conv_block(t4, f[4])\n",
    "\n",
    "    # Nested connections (U-Net++)\n",
    "    # Dense skip connections between the decoder and encoder\n",
    "    u1_0 = up_block(bn, c4, f[3])\n",
    "    u2_0 = up_block(u1_0, c3, f[2])\n",
    "    u3_0 = up_block(u2_0, c2, f[1])\n",
    "    u4_0 = up_block(u3_0, c1, f[0])\n",
    "\n",
    "    # Extra skip connections for U-Net++\n",
    "    u2_1 = up_block(u1_0, u2_0, f[2])\n",
    "    u3_1 = up_block(u2_1, u3_0, f[1])\n",
    "    u4_1 = up_block(u3_1, u4_0, f[0])\n",
    "\n",
    "    u3_2 = up_block(u2_1, u3_1, f[1])\n",
    "    u4_2 = up_block(u3_2, u4_1, f[0])\n",
    "\n",
    "    u4_3 = up_block(u3_2, u4_2, f[0])\n",
    "\n",
    "    # Final output layer\n",
    "    outputs = Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(u4_3)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs, outputs) \n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the U-Net++ model\n",
    "global_model = UNetPlusPlus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2e5235-f3af-49c9-8d84-eb66f0fa3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Train on Each Client\n",
    "def train_on_client(client_data, global_model, epochs=5, batch_size=32):\n",
    "    # Clone the global model for client-specific training\n",
    "    client_model = tf.keras.models.clone_model(global_model)\n",
    "    client_model.set_weights(global_model.get_weights())  # Initialize with global model weights\n",
    "\n",
    "    client_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                         loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "    client_generator = client_data.data_generator(batch_size=batch_size)\n",
    "    \n",
    "    # Fit the client model and get the training loss\n",
    "    history = client_model.fit(client_generator, epochs=epochs, steps_per_epoch=len(client_data.image_paths) // batch_size)\n",
    "\n",
    "    # Return weight updates and training loss\n",
    "    weight_updates = [client_model.get_weights()[i] - global_model.get_weights()[i] \n",
    "                      for i in range(len(global_model.get_weights()))]\n",
    "    return weight_updates, history.history['loss'][-1]  # Return final loss of the last epoch\n",
    "    \n",
    "\n",
    "# Federated Averaging to Aggregate the Weights\n",
    "def aggregate_models(global_model, client_weight_updates, num_clients):\n",
    "    # Average the weight updates from all clients\n",
    "    averaged_weight_updates = [np.mean([client_weight_updates[i][j] for i in range(num_clients)], axis=0) \n",
    "                               for j in range(len(global_model.get_weights()))]\n",
    "\n",
    "    # Apply the averaged weight updates to the global model\n",
    "    new_global_weights = [global_model.get_weights()[i] + averaged_weight_updates[i] \n",
    "                          for i in range(len(global_model.get_weights()))]\n",
    "    global_model.set_weights(new_global_weights)\n",
    "    \n",
    "\n",
    "# Federated Learning Process with Loss Tracking\n",
    "def federated_training(train_client_datasets, global_model, epochs=10, num_clients=5, batch_size=32):\n",
    "    \"\"\"\n",
    "    Federated training with client weight updates and loss tracking.\n",
    "\n",
    "    Args:\n",
    "        train_client_datasets: List of DataLoader instances for each client.\n",
    "        global_model: The global model to be trained.\n",
    "        epochs: Number of global training epochs.\n",
    "        num_clients: Number of clients.\n",
    "        batch_size: Batch size for training.\n",
    "\n",
    "    Returns:\n",
    "        Updated global model and list of global training losses.\n",
    "    \"\"\"\n",
    "    global_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                         loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "    global_losses = []  # Track the global loss after each epoch\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Global Training Epoch {epoch + 1}/{epochs}\")\n",
    "        client_weight_updates = []\n",
    "        epoch_loss = 0  # To accumulate loss from all clients\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            print(f\"Training on client {i + 1}/{num_clients}\")\n",
    "            # Train the client and collect weight updates (deltas) and loss\n",
    "            weight_updates, client_loss = train_on_client(train_client_datasets[i], global_model, epochs=1, batch_size=batch_size)\n",
    "            client_weight_updates.append(weight_updates)\n",
    "            epoch_loss += client_loss  # Accumulate client loss\n",
    "\n",
    "        # Aggregate the model using weight updates\n",
    "        aggregate_models(global_model, client_weight_updates, num_clients)\n",
    "        print(\"Updated global model.\")\n",
    "\n",
    "        # Average the loss across clients and store for plotting\n",
    "        avg_epoch_loss = epoch_loss / num_clients\n",
    "        global_losses.append(avg_epoch_loss)\n",
    "        print(f\"Average Loss for Epoch {epoch + 1}: {avg_epoch_loss}\")\n",
    "\n",
    "    return global_model, global_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef4b864-be40-4732-8ebb-4bb9dcc5613f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Standard FedAvg...\n",
      "Global Training Epoch 1/20\n",
      "Training on client 1/5\n",
      "394/394 [==============================] - 64s 129ms/step - loss: 0.4884 - accuracy: 0.8347\n",
      "Training on client 2/5\n",
      "394/394 [==============================] - 52s 107ms/step - loss: 0.5062 - accuracy: 0.7963\n",
      "Training on client 3/5\n",
      "394/394 [==============================] - 53s 109ms/step - loss: 0.5146 - accuracy: 0.8249\n",
      "Training on client 4/5\n",
      "394/394 [==============================] - 47s 95ms/step - loss: 0.5066 - accuracy: 0.7834\n",
      "Training on client 5/5\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function DataLoader.data_generator at 0x7f8043a01f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "396/396 [==============================] - 51s 105ms/step - loss: 0.5230 - accuracy: 0.7601\n",
      "Updated global model.\n",
      "Average Loss for Epoch 1: 0.5077752649784089\n",
      "Global Training Epoch 2/20\n",
      "Training on client 1/5\n",
      "394/394 [==============================] - 52s 107ms/step - loss: 0.2724 - accuracy: 0.9081\n",
      "Training on client 2/5\n",
      "394/394 [==============================] - 47s 96ms/step - loss: 0.3011 - accuracy: 0.8928\n",
      "Training on client 3/5\n",
      "394/394 [==============================] - 52s 107ms/step - loss: 0.3065 - accuracy: 0.8960\n",
      "Training on client 4/5\n",
      "394/394 [==============================] - 56s 113ms/step - loss: 0.3422 - accuracy: 0.8549\n",
      "Training on client 5/5\n",
      "396/396 [==============================] - 55s 109ms/step - loss: 0.3724 - accuracy: 0.8364\n",
      "Updated global model.\n",
      "Average Loss for Epoch 2: 0.3189369082450867\n",
      "Global Training Epoch 3/20\n",
      "Training on client 1/5\n",
      "394/394 [==============================] - 54s 108ms/step - loss: 0.2287 - accuracy: 0.9129\n",
      "Training on client 2/5\n",
      "394/394 [==============================] - 54s 110ms/step - loss: 0.2587 - accuracy: 0.8962\n",
      "Training on client 3/5\n",
      "394/394 [==============================] - 55s 111ms/step - loss: 0.2609 - accuracy: 0.8990\n",
      "Training on client 4/5\n",
      "394/394 [==============================] - 53s 104ms/step - loss: 0.3080 - accuracy: 0.8632\n",
      "Training on client 5/5\n",
      "396/396 [==============================] - 54s 109ms/step - loss: 0.3432 - accuracy: 0.8420\n",
      "Updated global model.\n",
      "Average Loss for Epoch 3: 0.2799034297466278\n",
      "Global Training Epoch 4/20\n",
      "Training on client 1/5\n",
      "394/394 [==============================] - 51s 103ms/step - loss: 0.2111 - accuracy: 0.9189\n",
      "Training on client 2/5\n",
      "394/394 [==============================] - 53s 110ms/step - loss: 0.2457 - accuracy: 0.9003\n",
      "Training on client 3/5\n",
      "394/394 [==============================] - 59s 123ms/step - loss: 0.2495 - accuracy: 0.9008\n",
      "Training on client 4/5\n",
      "394/394 [==============================] - 56s 111ms/step - loss: 0.2925 - accuracy: 0.8727\n",
      "Training on client 5/5\n",
      "396/396 [==============================] - 53s 108ms/step - loss: 0.3252 - accuracy: 0.8557\n",
      "Updated global model.\n",
      "Average Loss for Epoch 4: 0.2648042947053909\n",
      "Global Training Epoch 5/20\n",
      "Training on client 1/5\n",
      "394/394 [==============================] - 54s 111ms/step - loss: 0.2009 - accuracy: 0.9225\n",
      "Training on client 2/5\n",
      "394/394 [==============================] - 56s 116ms/step - loss: 0.2378 - accuracy: 0.9036\n",
      "Training on client 3/5\n",
      "394/394 [==============================] - 51s 102ms/step - loss: 0.2410 - accuracy: 0.9037\n",
      "Training on client 4/5\n",
      "394/394 [==============================] - 55s 108ms/step - loss: 0.2743 - accuracy: 0.8832\n",
      "Training on client 5/5\n",
      "396/396 [==============================] - 49s 99ms/step - loss: 0.3142 - accuracy: 0.8635\n",
      "Updated global model.\n",
      "Average Loss for Epoch 5: 0.2536345809698105\n",
      "Global Training Epoch 6/20\n",
      "Training on client 1/5\n",
      "394/394 [==============================] - 54s 111ms/step - loss: 0.1937 - accuracy: 0.9258\n",
      "Training on client 2/5\n",
      "394/394 [==============================] - 55s 113ms/step - loss: 0.2300 - accuracy: 0.9085\n",
      "Training on client 3/5\n",
      "394/394 [==============================] - 54s 110ms/step - loss: 0.2341 - accuracy: 0.9055\n",
      "Training on client 4/5\n",
      "394/394 [==============================] - 52s 106ms/step - loss: 0.2632 - accuracy: 0.8882\n",
      "Training on client 5/5\n",
      "396/396 [==============================] - 56s 108ms/step - loss: 0.3084 - accuracy: 0.8675\n",
      "Updated global model.\n",
      "Average Loss for Epoch 6: 0.24589018821716307\n",
      "Global Training Epoch 7/20\n",
      "Training on client 1/5\n",
      "394/394 [==============================] - 53s 107ms/step - loss: 0.1851 - accuracy: 0.9301\n",
      "Training on client 2/5\n",
      "394/394 [==============================] - 53s 109ms/step - loss: 0.2253 - accuracy: 0.9106\n",
      "Training on client 3/5\n",
      "394/394 [==============================] - 53s 108ms/step - loss: 0.2263 - accuracy: 0.9084\n",
      "Training on client 4/5\n",
      "394/394 [==============================] - 54s 109ms/step - loss: 0.2543 - accuracy: 0.8926\n",
      "Training on client 5/5\n",
      "396/396 [==============================] - 55s 112ms/step - loss: 0.2976 - accuracy: 0.8733\n",
      "Updated global model.\n",
      "Average Loss for Epoch 7: 0.23769829869270326\n",
      "Global Training Epoch 8/20\n",
      "Training on client 1/5\n",
      "394/394 [==============================] - 53s 110ms/step - loss: 0.1801 - accuracy: 0.9325\n",
      "Training on client 2/5\n",
      "394/394 [==============================] - 57s 112ms/step - loss: 0.2182 - accuracy: 0.9139\n",
      "Training on client 3/5\n",
      "394/394 [==============================] - 53s 107ms/step - loss: 0.2218 - accuracy: 0.9107\n",
      "Training on client 4/5\n",
      "394/394 [==============================] - 54s 110ms/step - loss: 0.2471 - accuracy: 0.8977\n",
      "Training on client 5/5\n",
      "396/396 [==============================] - 56s 115ms/step - loss: 0.2854 - accuracy: 0.8800\n",
      "Updated global model.\n",
      "Average Loss for Epoch 8: 0.23050535023212432\n",
      "Global Training Epoch 9/20\n",
      "Training on client 1/5\n",
      "394/394 [==============================] - 55s 115ms/step - loss: 0.1740 - accuracy: 0.9342\n",
      "Training on client 2/5\n",
      "394/394 [==============================] - 57s 112ms/step - loss: 0.2171 - accuracy: 0.9153\n",
      "Training on client 3/5\n",
      "394/394 [==============================] - 55s 112ms/step - loss: 0.2170 - accuracy: 0.9118\n",
      "Training on client 4/5\n",
      "394/394 [==============================] - 56s 115ms/step - loss: 0.2413 - accuracy: 0.9017\n",
      "Training on client 5/5\n",
      "396/396 [==============================] - 59s 119ms/step - loss: 0.2806 - accuracy: 0.8823\n",
      "Updated global model.\n",
      "Average Loss for Epoch 9: 0.225997993350029\n",
      "Global Training Epoch 10/20\n",
      "Training on client 1/5\n",
      "394/394 [==============================] - 59s 112ms/step - loss: 0.1700 - accuracy: 0.9358\n",
      "Training on client 2/5\n",
      "394/394 [==============================] - 54s 111ms/step - loss: 0.2095 - accuracy: 0.9180\n",
      "Training on client 3/5\n",
      "394/394 [==============================] - 54s 110ms/step - loss: 0.2104 - accuracy: 0.9144\n",
      "Training on client 4/5\n",
      "394/394 [==============================] - 51s 105ms/step - loss: 0.2353 - accuracy: 0.9034\n",
      "Training on client 5/5\n",
      "396/396 [==============================] - 55s 111ms/step - loss: 0.2715 - accuracy: 0.8882\n",
      "Updated global model.\n",
      "Average Loss for Epoch 10: 0.21936228275299072\n",
      "Global Training Epoch 11/20\n",
      "Training on client 1/5\n",
      "394/394 [==============================] - 52s 109ms/step - loss: 0.1641 - accuracy: 0.9378\n",
      "Training on client 2/5\n",
      "394/394 [==============================] - 54s 109ms/step - loss: 0.2072 - accuracy: 0.9190\n",
      "Training on client 3/5\n",
      "394/394 [==============================] - 55s 112ms/step - loss: 0.2078 - accuracy: 0.9160\n",
      "Training on client 4/5\n",
      "394/394 [==============================] - 55s 110ms/step - loss: 0.2263 - accuracy: 0.9084\n",
      "Training on client 5/5\n",
      "396/396 [==============================] - 51s 101ms/step - loss: 0.2640 - accuracy: 0.8912\n",
      "Updated global model.\n",
      "Average Loss for Epoch 11: 0.21385472118854523\n",
      "Global Training Epoch 12/20\n",
      "Training on client 1/5\n",
      "394/394 [==============================] - 53s 110ms/step - loss: 0.1587 - accuracy: 0.9397\n",
      "Training on client 2/5\n",
      "394/394 [==============================] - 65s 124ms/step - loss: 0.2009 - accuracy: 0.9218\n",
      "Training on client 3/5\n",
      "394/394 [==============================] - 52s 104ms/step - loss: 0.2049 - accuracy: 0.9160\n",
      "Training on client 4/5\n",
      "394/394 [==============================] - 50s 103ms/step - loss: 0.2233 - accuracy: 0.9097\n",
      "Training on client 5/5\n",
      "396/396 [==============================] - 50s 102ms/step - loss: 0.2664 - accuracy: 0.8924\n",
      "Updated global model.\n",
      "Average Loss for Epoch 12: 0.21083832979202272\n",
      "Global Training Epoch 13/20\n",
      "Training on client 1/5\n",
      "394/394 [==============================] - 52s 104ms/step - loss: 0.1550 - accuracy: 0.9418\n",
      "Training on client 2/5\n",
      "394/394 [==============================] - 54s 111ms/step - loss: 0.1979 - accuracy: 0.9215\n",
      "Training on client 3/5\n",
      "394/394 [==============================] - 55s 111ms/step - loss: 0.2040 - accuracy: 0.9154\n",
      "Training on client 4/5\n",
      "394/394 [==============================] - 67s 143ms/step - loss: 0.2177 - accuracy: 0.9134\n",
      "Training on client 5/5\n",
      " 61/396 [===>..........................] - ETA: 23:24 - loss: 0.2527 - accuracy: 0.8947"
     ]
    }
   ],
   "source": [
    "# Initialize new model for tandard FedAvg\n",
    "global_fedavg =UNetPlusPlus()\n",
    "\n",
    "# Run standard FedAvg\n",
    "print(\"Running Standard FedAvg...\") \n",
    "start_time = time.time()\n",
    "# Train the global model with FedAvg \n",
    "fedavg_model, fedavg_loss =  federated_training(train_client_datasets, \n",
    "                                                global_model, epochs=epochs, \n",
    "                                                num_clients=num_clients, batch_size=batch_size)\n",
    "end_time = time.time()\n",
    "\n",
    "# Store training time for Standard FedAvg\n",
    "fl_training_times = end_time - start_time\n",
    "\n",
    "print(f\"Standard FedAvg training time: {fl_training_times:.2f} seconds\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e73edd-f3d1-4f4d-b821-b6adc5e45dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# BB84 Quantum Key Distribution Protocol\n",
    "class BB84:\n",
    "    def __init__(self, key_length=128):\n",
    "        self.key_length = key_length\n",
    "\n",
    "    def generate_key(self):\n",
    "        bits = [random.choice([0, 1]) for _ in range(self.key_length)]\n",
    "        bases = [random.choice([0, 1]) for _ in range(self.key_length)]\n",
    "        return bits, bases\n",
    "\n",
    "    def receive_key(self, alice_bits, alice_bases, client_bases):\n",
    "        key = [alice_bits[i] for i in range(self.key_length) if alice_bases[i] == client_bases[i]]\n",
    "        return key\n",
    "\n",
    "# Fully Homomorphic Encryption \n",
    "class FullyHomomorphicEncryption:\n",
    "    def __init__(self, poly_modulus_degree=16384*2):  # Increased degree\n",
    "        self.context = ts.context(ts.SCHEME_TYPE.CKKS, poly_modulus_degree=poly_modulus_degree, coeff_mod_bit_sizes=[60, 40, 40, 60])\n",
    "        self.context.generate_galois_keys()\n",
    "        self.context.global_scale = 2**40\n",
    "\n",
    "    def set_key(self, key):\n",
    "        # Optionally use the key to further personalize encryption\n",
    "        self.context.global_scale = 2**40  # Example of using a constant global scale\n",
    "\n",
    "    def encrypt(self, tensor):\n",
    "        if isinstance(tensor, np.ndarray):\n",
    "            flat_tensor = tensor.flatten()\n",
    "            encrypted_tensor = ts.ckks_vector(self.context, flat_tensor)\n",
    "            return encrypted_tensor\n",
    "        else:\n",
    "            raise ValueError(\"Input should be a numpy array\")\n",
    "\n",
    "    def decrypt(self, encrypted_tensor, shape):\n",
    "        flat_tensor = encrypted_tensor.decrypt()\n",
    "        return np.reshape(flat_tensor, shape)\n",
    "\n",
    "    def add_encrypted(self, enc1, enc2):\n",
    "        return enc1 + enc2\n",
    "\n",
    "    def multiply_encrypted(self, enc1, scalar):\n",
    "        return enc1 * scalar\n",
    "\n",
    "def train_on_client(client_data, global_model, epochs=5, batch_size=32):\n",
    "    # Clone the global model for client-specific training\n",
    "    client_model = tf.keras.models.clone_model(global_model)\n",
    "    client_model.set_weights(global_model.get_weights())  # Initialize with global model weights\n",
    "\n",
    "    client_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                         loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "    client_generator = client_data.data_generator(batch_size=batch_size)\n",
    "    \n",
    "    # Fit the client model and get the training loss\n",
    "    history = client_model.fit(client_generator, epochs=epochs, steps_per_epoch=len(client_data.image_paths) // batch_size)\n",
    "\n",
    "    # Return weight updates and training loss\n",
    "    weight_updates = [client_model.get_weights()[i] - global_model.get_weights()[i] \n",
    "                      for i in range(len(global_model.get_weights()))]\n",
    "    return weight_updates, history.history['loss'][-1]  # Return final loss of the last epoch\n",
    "\n",
    "# Federated Learning Process with FHE and BB84\n",
    "def qce_training(train_client_datasets, global_model, epochs=10, num_clients=5, batch_size=32, key_length=128):\n",
    "    \"\"\"\n",
    "    Federated training with BB84 quantum key distribution and Fully Homomorphic Encryption.\n",
    "\n",
    "    Args:\n",
    "        train_client_datasets: List of DataLoader instances for each client.\n",
    "        global_model: The global model to be trained.\n",
    "        epochs: Number of global training epochs.\n",
    "        num_clients: Number of clients.\n",
    "        batch_size: Batch size for training.\n",
    "        key_length: Length of the quantum key (default is 128 bits).\n",
    "\n",
    "    Returns:\n",
    "        Updated global model and list of global training losses.\n",
    "    \"\"\"\n",
    "    global_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                         loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "    global_losses = []  # Track the global loss after each epoch\n",
    "    bb84_protocol = BB84(key_length)  # Initialize BB84 protocol for quantum key distribution\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Global Training Epoch {epoch + 1}/{epochs}\")\n",
    "        client_weight_updates = []\n",
    "        epoch_loss = 0  # To accumulate loss from all clients\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            print(f\"Training on client {i + 1}/{num_clients}\")\n",
    "            # Step 1: Alice (server) generates bits and bases for BB84\n",
    "            alice_bits, alice_bases = bb84_protocol.generate_key()\n",
    "\n",
    "            # Step 2: Client generates its bases and receives the key\n",
    "            client_bases = [random.choice([0, 1]) for _ in range(key_length)]\n",
    "            client_key = bb84_protocol.receive_key(alice_bits, alice_bases, client_bases)\n",
    "\n",
    "            # Step 3: Initialize Fully Homomorphic Encryption with the client's key\n",
    "            fhe = FullyHomomorphicEncryption()\n",
    "            fhe.set_key(client_key)\n",
    "\n",
    "            # Step 4: Train the client and collect weight updates (deltas) and loss\n",
    "            weight_updates, client_loss = train_on_client(train_client_datasets[i], global_model, epochs=1, batch_size=batch_size)\n",
    "            epoch_loss += client_loss  # Accumulate client loss\n",
    "\n",
    "            # Step 5: Encrypt the weight updates using FHE\n",
    "            #encrypted_weight_updates = [fhe.encrypt(np.array(w)) for w in weight_updates]\n",
    "            encrypted_weight_updates = [fhe.encrypt_in_batches(np.array(w), batch_size=batch_size) for w in weight_updates]\n",
    "            client_weight_updates.append(encrypted_weight_updates)\n",
    "\n",
    "        # Step 6: Aggregate the encrypted weight updates using FHE without decrypting them\n",
    "        encrypted_aggregated_updates = []\n",
    "        for j in range(len(global_model.get_weights())):\n",
    "            enc_sum = client_weight_updates[0][j]  # Start with the first client's update\n",
    "            for k in range(1, num_clients):\n",
    "                enc_sum = fhe.add_encrypted(enc_sum, client_weight_updates[k][j])  # Add updates in encrypted space\n",
    "            encrypted_aggregated_updates.append(fhe.multiply_encrypted(enc_sum, 1 / num_clients))  # Average\n",
    "\n",
    "        # Step 7: Decrypt aggregated weight updates and apply them to the global model\n",
    "        decrypted_updates = [fhe.decrypt(enc_update, global_model.get_weights()[j].shape) for j, enc_update in enumerate(encrypted_aggregated_updates)]\n",
    "        new_global_weights = [global_model.get_weights()[i] + decrypted_updates[i] for i in range(len(global_model.get_weights()))]\n",
    "        global_model.set_weights(new_global_weights)\n",
    "\n",
    "        print(\"Updated global model.\")\n",
    "\n",
    "        # Average the loss across clients and store for plotting\n",
    "        avg_epoch_loss = epoch_loss / num_clients\n",
    "        global_losses.append(avg_epoch_loss)\n",
    "        print(f\"Average Loss for Epoch {epoch + 1}: {avg_epoch_loss}\")\n",
    "\n",
    "    return global_model, global_losses\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebec788-9721-45a8-8b19-f66521304b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new model for QKD and FHE\n",
    "global_qcefl = UNetPlusPlus()\n",
    "    \n",
    "# Run FedAvg with QKD and FHE\n",
    "print(\"QKD-FHE...\")\n",
    "    \n",
    "start_time = time.time()\n",
    "# Train the global model with FedAvg\n",
    "qce_fl_model, qce_loss = qce_training(train_client_datasets, \n",
    "                                                global_model, epochs=epochs, \n",
    "                                                num_clients=num_clients, batch_size=batch_size, key_length=128)\n",
    "end_time = time.time()\n",
    "\n",
    "# Store training time for FedAvg with QKD and FHE\n",
    "qce_training_times = end_time - start_time\n",
    "\n",
    "print(f\"QCE-FL training time: {qce_training_times:.2f} seconds\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d9770b-1814-494a-8856-9351f577abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "epochs = range(1, len(qce_loss) + 1)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(epochs, fedavg_loss, label='FedAvg Loss', marker='o', color='blue')\n",
    "plt.plot(epochs, qce_loss, label='QCE-FL Loss', marker='o', color='red')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "#plt.title('Training Loss Comparison: QCE-FL vs FedAvg')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0016ec29-8fcc-4a69-a54e-38ab8386c640",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "def plot_computational_burden(fl_training_times, qce_training_times):\n",
    "    # Define labels and values\n",
    "    labels = ['FedAvg', 'QCE-FL']\n",
    "    values = [fl_training_times, qce_training_times]\n",
    "    \n",
    "    # Define colors\n",
    "    colors = ['#D3D3D3', '#FFFFE0']  # Light ash and light yellow\n",
    "\n",
    "    # Create the bar chart\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(labels, values, color=colors)\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title('Computational Burden: FedAvg vs QCE-FL')\n",
    "    plt.ylabel('Training Time (seconds)')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "#fl_training_times = 50  # Example training time for FedAvg\n",
    "#qce_training_times = 120  # Example training time for QCE-FL\n",
    "plot_computational_burden(fl_training_times, qce_training_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c291011c-b296-47d8-a280-4c35eae05586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a combined test dataset\n",
    "def create_combined_test_dataset(test_client_datasets):\n",
    "    combined_image_paths = []\n",
    "    combined_mask_paths = []\n",
    "\n",
    "    # Iterate over each client's test dataset to combine the paths\n",
    "    for client_data in test_client_datasets:\n",
    "        combined_image_paths.extend(client_data.image_paths)\n",
    "        combined_mask_paths.extend(client_data.mask_paths)\n",
    "\n",
    "    # Create a new DataLoader for the combined test dataset\n",
    "    combined_dataset = DataLoader(combined_image_paths, combined_mask_paths, augment=False)\n",
    "    return combined_dataset\n",
    "\n",
    " \n",
    "# Create the combined test dataset\n",
    "combined_test_dataset = create_combined_test_dataset(test_client_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54799b2e-74c0-4f8a-b56e-ec4f934a63cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract ground truth masks from the combined test dataset\n",
    "def extract_ground_truth_masks(data_loader):\n",
    "    masks = []\n",
    "\n",
    "    for image_batch, mask_batch in data_loader.data_generator(batch_size=1, shuffle=False):\n",
    "        masks.append(mask_batch.numpy())  # Convert tensor to numpy array\n",
    "\n",
    "    # Stack the masks to form a single array\n",
    "    return np.vstack(masks)  # Use vstack to concatenate along the first axis\n",
    "\n",
    "# Get ground truth masks\n",
    "ground_truth_masks = extract_ground_truth_masks(combined_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8743e35e-7ae0-4cff-9ad4-beb4a8e970ed",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "def dice_score(predictions, masks, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate the DICE score for the given predictions and ground truth masks.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Model predictions (numpy array).\n",
    "        masks: Ground truth masks (numpy array).\n",
    "        threshold: Threshold to binarize predictions.\n",
    "    \n",
    "    Returns:\n",
    "        DICE score.\n",
    "    \"\"\"\n",
    "    predictions_binary = (predictions > threshold).astype(np.float32)\n",
    "    intersection = np.sum(predictions_binary * masks)\n",
    "    return 2 * intersection / (np.sum(predictions_binary) + np.sum(masks) + 1e-8)  # Add epsilon to avoid division by zero\n",
    "\n",
    "# Predicting on the test datasets\n",
    "def predict_on_test(test_data, global_model, batch_size=32):\n",
    "    test_generator = test_data.data_generator(batch_size=batch_size, shuffle=False)\n",
    "    predictions = global_model.predict(test_generator)\n",
    "    return predictions\n",
    "\n",
    "\n",
    " \n",
    "# Get predictions for both models\n",
    "predictions_fedavg = predict_on_test(combined_test_dataset, fedavg_model)\n",
    "predictions_qce_fl = predict_on_test(combined_test_dataset, qce_fl_model)\n",
    "\n",
    "# Calculate DICE scores\n",
    "dice_score_fedavg = dice_score(predictions_fedavg, ground_truth_masks)\n",
    "dice_score_qce_fl = dice_score(predictions_qce_fl, ground_truth_masks)\n",
    "\n",
    "# Plot DICE scores\n",
    "models = ['FedAvg', 'QCE-FL']\n",
    "dice_scores = [dice_score_fedavg, dice_score_qce_fl]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(models, dice_scores, color=['lightyellow', 'lightcoral'])\n",
    "plt.ylabel('DICE Score')\n",
    "#plt.title('DICE Scores of the Models')\n",
    "plt.ylim(0, 1)  # DICE scores range from 0 to 1\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279a60a-cf3e-449d-ac28-36bcda846649",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_models = {\n",
    "    'FedAvg': fedavg_model,\n",
    "    'QCE-FL': qce_fl_model\n",
    "}\n",
    "\n",
    "# Assuming combined_test_dataset is your DataLoader instance\n",
    "# Make sure you get the original images in the correct format\n",
    "original_images = [test_data[0] for test_data in combined_test_dataset.data_generator(batch_size=len(combined_test_dataset.image_paths), shuffle=False)]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3bb11a-1f5b-4321-b6e6-c64dcee0c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "def visualize_results(images, ground_truth, models, num_samples=2):\n",
    "    \"\"\"\n",
    "    Visualizes the segmentation results for the specified number of samples.\n",
    "\n",
    "    Args:\n",
    "        images (list): List of original images.\n",
    "        ground_truth (np.ndarray): Array of ground truth masks.\n",
    "        models (dict): Dictionary of models to evaluate.\n",
    "        num_samples (int): Number of samples to visualize.\n",
    "    \"\"\"\n",
    "    num_models = len(models)\n",
    "\n",
    "    # Determine how many samples to visualize based on available data\n",
    "    num_samples = min(num_samples, len(images))\n",
    "\n",
    "    # Create a figure with adjusted spacing\n",
    "    fig, axes = plt.subplots(\n",
    "        num_samples, \n",
    "        num_models + 2,  # 2 for original and ground truth, plus models\n",
    "        figsize=(15, 5 * num_samples), \n",
    "        gridspec_kw={'wspace': 0.01, 'hspace': 0.1}\n",
    "    )\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Convert image to numpy array (assuming images are still Tensors)\n",
    "        image_np = images[i].numpy() if isinstance(images[i], tf.Tensor) else images[i]\n",
    "        ground_truth_np = ground_truth[i]  # No need to call .numpy() here\n",
    "\n",
    "        # Display original image\n",
    "        ax = axes[i, 0] if num_samples > 1 else axes[0]\n",
    "        ax.imshow(image_np[0], cmap='gray')  # Display the first image in the batch\n",
    "        ax.axis('off')\n",
    "        if i == num_samples - 1:\n",
    "            ax.set_title(\"Image\", y=-0.2, fontsize=14)\n",
    "        \n",
    "        # Display ground truth mask overlaid on the original image\n",
    "        ax = axes[i, 1] if num_samples > 1 else axes[1]\n",
    "        ax.imshow(image_np[0], cmap='gray')  # Show original image\n",
    "        ax.imshow(ground_truth_np.squeeze(), cmap='jet', alpha=0.5)  # Overlay mask\n",
    "        ax.axis('off')\n",
    "        if i == num_samples - 1:\n",
    "            ax.set_title(\"GT\", y=-0.2, fontsize=14)\n",
    "        \n",
    "        # Display predicted masks overlaid on the original image for each model\n",
    "        for j, (model_name, model) in enumerate(models.items()):\n",
    "            # Predict and ensure it is a numpy array\n",
    "            predicted_mask = model.predict(tf.expand_dims(image_np[0], axis=0), verbose=0)[0]\n",
    "            if isinstance(predicted_mask, tf.Tensor):\n",
    "                predicted_mask = predicted_mask.numpy()\n",
    "            \n",
    "            ax = axes[i, j + 2] if num_samples > 1 else axes[j + 2]\n",
    "            ax.imshow(image_np[0], cmap='gray')  # Show original image\n",
    "            ax.imshow(predicted_mask.squeeze(), cmap='jet', alpha=0.5)  # Overlay predicted mask\n",
    "            ax.axis('off')\n",
    "            if i == num_samples - 1:\n",
    "                ax.set_title(model_name, y=-0.2, fontsize=14)\n",
    "            \n",
    "    # Adjust subplot spacing\n",
    "    plt.subplots_adjust(wspace=0.1, top=0.9, hspace=0.1)\n",
    "    plt.show()\n",
    "\n",
    " \n",
    "# Call the function to visualize results\n",
    "visualize_results(original_images, ground_truth_masks, fl_models, num_samples=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0da3cb-d3a7-42b0-a1fe-df90275b62d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a83c5d-782b-43ec-9a24-3d3628e6b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e883da-2c8e-405b-b15c-e2c4687fbf8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
